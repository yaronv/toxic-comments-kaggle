{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comments Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaron/python-envs/env-3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word2Vec/Glove/FastText Words Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_dimension = 0 # set this in each method\n",
    "max_features = 30000    # number of unique tokens that the tokenizer save\n",
    "maxlen = 25             # maximum sequence length\n",
    "\n",
    "## Loading Wprd2Vec Data\n",
    "embeddings_type = \"FAST_TEXT\"\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "if embeddings_type == \"GLOVE\":\n",
    "    embedding_dimension = 200\n",
    "    glove_data = '../../data/glove.6B/glove.6B.200d.txt'\n",
    "    f = open(glove_data)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        value = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = value\n",
    "    f.close()\n",
    "elif embeddings_type == \"WORD2VEC\":\n",
    "    embedding_dimension = 300\n",
    "    google_word2vec = '../../data/GoogleNews-vectors-negative300.bin.gz'\n",
    "    vocab_model = gensim.models.KeyedVectors.load_word2vec_format(google_word2vec, binary=True)\n",
    "    embedding_matrix = vocab_model.self.vectors\n",
    "    vocab_dict = {word: embedding_matrix[vocab_model.vocab[word].index] for word in vocab_model.vocab.keys()}\n",
    "    embeddings_index = vocab_dict\n",
    "elif embeddings_type == \"FAST_TEXT\":\n",
    "    embedding_dimension = 300\n",
    "    file = '../../data/crawl-300d-2M.vec'\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(file))\n",
    "\n",
    "print('Loaded %s word vectors using %s.' % (len(embeddings_index), embeddings_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization helper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('R'):\n",
    "            yield wnl.lemmatize(word, pos='r')\n",
    "            \n",
    "        else:\n",
    "            yield word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train/Test Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!\n",
      "De jure, there's no legitimate government. After abu Mazen dismissed the Hamas government, he was the legitimate government, but only for the short period allowed by the PA constitution after which new elections were necessary. He ignored that provision. Both governments are illegitimate, although the Hamas government in Gaza was at least elected, while Abbas is basically a Washington/Tel Aviv appointee, FWIW.\n",
      "???????????????\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")\n",
    "train = train.sample(frac=1)\n",
    "\n",
    "X_train_sentences = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "X_test_sentences = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "print(\"!!!!!!!!!!!!!!!!\")\n",
    "print(X_train_sentences[0])\n",
    "print(\"???????????????\")\n",
    "\n",
    "X_train_sentences = np.asarray([\" \".join(lemmatize_all(x)) for x in X_train_sentences])\n",
    "X_test_sentences = np.asarray([\" \".join(lemmatize_all(x)) for x in X_test_sentences])\n",
    "print(X_train_sentences[0])\n",
    "print(\"@@@@@@@@@@@@@@@\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train[list_classes].values\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n",
    "tokenizer.fit_on_texts(list(X_train_sentences))\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train_sentences)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(X_test_sentences)\n",
    "\n",
    "X_train_sequences = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test_sequences = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Create embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210338, 300)\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "## Generate embeddings matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimension))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector[:embedding_dimension]\n",
    "\n",
    "print(str(embedding_matrix.shape))\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695ce28ba7f342b4bd5edf9cd9a39529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d376af87ef27443f81f0f1aed998e96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=143613), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05446, saving model to weights_base.best.hdf5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a03d53d2f44465a9232b4d67ae0009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=143613), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss improved from 0.05446 to 0.05223, saving model to weights_base.best.hdf5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = embedding_layer(inp)X_te\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True))(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True))(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = Bidirectional(LSTM(100))(x)\n",
    "#     x = Dense(100, activation=\"tanh\")(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    optimizer = optimizers.Adagrad(lr=0.01, clipvalue=0.5)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "file_path=\"weights_base.best.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint, early, TQDMNotebookCallback()]\n",
    "model.fit(X_train_sequences, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "model.load_weights(file_path)\n",
    "\n",
    "y_test = model.predict(X_test_sequences)\n",
    "\n",
    "\n",
    "\n",
    "sample_submission = pd.read_csv(\"input/sample_submission.csv\")\n",
    "\n",
    "sample_submission[list_classes] = y_test\n",
    "\n",
    "\n",
    "\n",
    "sample_submission.to_csv(\"baseline.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
